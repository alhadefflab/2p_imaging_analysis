{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all necessary libraries\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from tifffile import imread, imsave\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import median_filter\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import imageio\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect, tile_and_correct, motion_correction_piecewise\n",
    "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
    "from caiman.source_extraction.cnmf import params as params\n",
    "from cellpose import models\n",
    "from tqdm import tqdm\n",
    "from tifffile import imread, imsave\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy import stats as st\n",
    "from skimage.morphology import dilation\n",
    "from pystackreg import StackReg\n",
    "import pystackreg\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee180a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@figure settings\n",
    "mpl.rcParams.update({\n",
    "    'figure.subplot.wspace': .1,\n",
    "    'figure.subplot.hspace': .1,\n",
    "    'figure.figsize': (18, 13),\n",
    "    'svg.fonttype':'none'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file loading setting\n",
    "frame_function = get_cycle # the function for extracting the frame\n",
    "z_function = get_end# the function for extracting the z level\n",
    "z = 'z1' # the z level of interest for this analysis\n",
    "f_ch = 'ch2' # the functional channel \n",
    "mc_ch = 'ch1' # the channel to use for motion correction\n",
    "ds = False # whether or not to downsample the movie spatially\n",
    "step_size = 2 # number of pixels to skip when downsampling\n",
    "remake = False # whether or not to remake the downsampled and cropped movies\n",
    "frame_period = 0.585 # time in seconds between frames, get this from the xml file\n",
    "\n",
    "# for the videos\n",
    "ds_ratio = 0.5 # the fraction of frames to show in the video\n",
    "speed_up = 20  # how much to speed up the video by\n",
    "\n",
    "##############################################################################################\n",
    "fr = 1/frame_period # frame rate\n",
    "fps = speed_up * fr * ds_ratio # frames per second for writing the video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdcb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cropping and concatenating multiple files\n",
    "from collections import OrderedDict\n",
    "multi_path = [Path('Path').resolve(), \n",
    "              Path('Path').resolve()]\n",
    "multi_crop = True\n",
    "multi_crop_start = [ , , ]\n",
    "multi_crop_len = [ , , ]\n",
    "multi_sp = {}\n",
    "concat_filenames = {}\n",
    "ch_list = [mc_ch,f_ch]\n",
    "multi_data = {}\n",
    "\n",
    "for ch in ch_list:\n",
    "    multi_data[ch] = OrderedDict()\n",
    "    for count, session in enumerate(multi_path):\n",
    "        print(\"session \"+str(count))\n",
    "        print(session.as_posix())\n",
    "        multi_sp[count]  = combine_tiffs(session, get_frame_cycle = frame_function, get_z = z_function)\n",
    "        if multi_crop:\n",
    "                p = multi_sp[count][z][ch]\n",
    "                data = imread(p).squeeze()\n",
    "                multi_data[ch][count] = data[multi_crop_start[count]:multi_crop_start[count] + multi_crop_len[count], :, :]                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a0a54",
   "metadata": {},
   "source": [
    "# Optional step to perform PyStackReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b983adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell iterates over the sessions that were defined in multi_path above.\n",
    "# For each session, a reference image is generated by averating the middle 50% of frames.\n",
    "# The reference image is appended to the beginning of the stack and the StackReg affine motion correction\n",
    "# runs using this images as the reference.\n",
    "# Then we apply the transform discovered with the motion correction channel to the mc channel and the functional channel\n",
    "# Then we chop the first frame back off and we ought to have affine motion corrected tiffs for both channels for all sessions.\n",
    "\n",
    "out_mcch = {}\n",
    "out_fch = {}\n",
    "\n",
    "affine_data = {}\n",
    "for ch in ch_list:\n",
    "    affine_data[ch] = OrderedDict()\n",
    "\n",
    "for count, session in enumerate(multi_path):\n",
    "    refsize = np.ceil(np.size(multi_data[mc_ch][count],0)/2)\n",
    "    # create 'ref' which is the mean of the middle 50% of images (approximately)\n",
    "    ref = np.mean(multi_data[mc_ch][count][int(np.ceil(refsize/2)):int(np.ceil(refsize/2)+refsize),:,:],0)\n",
    "    toreg_mc = multi_data[mc_ch][count]\n",
    "    toreg_fc = multi_data[f_ch][count]\n",
    "    firstframetrick_mc = np.vstack([ref[np.newaxis,...], toreg_mc]) # have to put ref as first frame because sr will not accept a reference image I created\n",
    "    firstframetrick_fc = np.vstack([ref[np.newaxis,...], toreg_fc]) # have to put ref as first frame because sr will not accept a reference image I created\n",
    "\n",
    "    #Affine transformation\n",
    "    sr = StackReg(StackReg.AFFINE)\n",
    "    out_aff = sr.register_stack(firstframetrick_mc, reference='first')\n",
    "    out_mcch[count] = sr.transform_stack(firstframetrick_mc)\n",
    "    out_fch[count] = sr.transform_stack(firstframetrick_fc)\n",
    "    \n",
    "    affine_data[mc_ch][count] = out_mcch[count][1:,:,:]\n",
    "    affine_data[f_ch][count] = out_fch[count][1:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = {}\n",
    "\n",
    "# This cell takes the affine transformed data and concatenates it all into one long movie across all sessions.\n",
    "for ch in ch_list:\n",
    "    concat_data = np.concatenate(list(affine_data[ch].values()))\n",
    "    p = multi_sp[count][z][ch]\n",
    "    fname = Path(p).parents[2] / Path(Path(p).parts[-2]+'_concat') / 'concat_cropped_affine.ome.tif'\n",
    "    mov_dir = Path(fname.parent)\n",
    "    concat_filenames[ch] = fname\n",
    "    if not mov_dir.exists(): mov_dir.mkdir(parents = True)\n",
    "    imsave(fname, concat_data)\n",
    "sp[z] = concat_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1efe36",
   "metadata": {},
   "source": [
    "# Rigid Motion Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c193fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for motion correction\n",
    "mc_params = {\n",
    "    \"max_shifts\": (50, 50),  # maximum allowed rigid shift in pixels (view the movie to get a sense of motion)\n",
    "    \"niter_rig\" :  3,          # number of times to perform rigid registration (i'd recommend doing at least 2 especially for these shorter recordings) # number of chunks for parallel processing ()\n",
    "    \"pw_rigid\"  : True,       # flag for performing rigid or piecewise rigid motion correction (false for now as we want to asses rigid registration first)\n",
    "    \"shifts_opencv\": True ,    # flag for correcting motion using bicubic interpolation (otherwise FFT interpolation is used)\n",
    "    \"border_nan\"   : 'copy',\n",
    "    \"nonneg_movie\" : True,\n",
    "    \"use_cuda\"  : False,\n",
    "    \"strides\"   : (64, 64), # size of patches for nonrigid motion correction in pixels\n",
    "    \"overlaps\"  : (32, 32), # number of pixrls of overlap between patches\n",
    "    \"max_deviation_rigid\"  : 3, # maximum allowed deviation of any individual patch's registered shift from the rigid shift\n",
    "    \"upsample_factor_grid\" : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize what different patch sizes will sample\n",
    "raw2 = cm.load_movie_chain([sp[z][mc_ch]])\n",
    "i,j=2,2 #rough patch index (ignoring overlaps)\n",
    "x, y = mc_params['strides'] # stride\n",
    "_,ax = plt.subplots(1,1)\n",
    "ax.imshow(raw2[0, i*y:(i+1)*y + 1, j*x:(j + 1)*x + 1]);\n",
    "del raw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up clusters for parallel processing\n",
    "if 'dview' in locals():\n",
    "    cm.stop_server(dview=dview)\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='local', n_processes=None, single_thread=False)\n",
    "#create motion correction object and perform rigid motion correction\n",
    "mc = MotionCorrect([sp[z][mc_ch]], dview=dview, **mc_params)\n",
    "mc.motion_correct(save_movie=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388092e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title plot shifts\n",
    "plt.plot(np.array(mc.shifts_rig)[:,0], label = 'x shifts');\n",
    "plt.plot(np.array(mc.shifts_rig)[:,1], label = 'y shifts');\n",
    "plt.legend();\n",
    "plt.xlabel('frames');\n",
    "plt.ylabel('pixels');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b01bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mc.pw_rigid:\n",
    "    _,ax=plt.subplots(2,1)\n",
    "    ax[0].plot(np.array(mc.x_shifts_els) - np.array(mc.shifts_rig)[:,0,None]);\n",
    "    ax[0].set_title('x shift deviations');\n",
    "    ax[1].plot(np.array(mc.y_shifts_els) - np.array(mc.shifts_rig)[:,1,None]);\n",
    "    ax[1].set_title('y shift deviations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr2 = np.array(cm.load(mc.mmap_file))\n",
    "raw2 = np.array(cm.load_movie_chain([sp[z][mc_ch]]))\n",
    "\n",
    "c = corr2.reshape(corr2.shape[0], -1)\n",
    "r = raw2.reshape(raw2.shape[0], -1)\n",
    "\n",
    "def corr_m(x):\n",
    "    xm = x.mean(axis=0)\n",
    "    xm_ = xm - xm.mean()\n",
    "    x_ = x - x.mean(axis=1)[:,None]\n",
    "    corr = (x_ @ xm_) / np.sqrt((x_ @ x_.T).diagonal() * (xm_**2).sum())\n",
    "    return corr\n",
    "\n",
    "c = corr_m(c)\n",
    "r = corr_m(r)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (10,5))\n",
    "ax[0].plot(c, label = 'corrected')\n",
    "ax[0].plot(r, label = 'raw')\n",
    "ax[0].set(xlabel = 'Frame', ylabel = 'Correlation with Mean')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].scatter(x = r, y= c)\n",
    "xlim = ax[1].get_xlim()\n",
    "ylim = ax[1].get_ylim()\n",
    "nlim = ( min(xlim[0], ylim[0]), max(xlim[1], ylim[1]) )\n",
    "ax[1].plot(nlim, nlim, color = 'k', ls = '--')\n",
    "ax[1].set( ylabel = 'Corrected', xlabel = 'Raw', \n",
    "           title = 'Correlation with Mean', \n",
    "           xlim = nlim, ylim = nlim)\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout(pad = 1.)\n",
    "\n",
    "del corr2\n",
    "del raw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ebb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear up some memory\n",
    "if 'c' in locals():del c\n",
    "if 'cl' in locals(): del c1\n",
    "if 'c2' in locals(): del c2\n",
    "if 'data_url' in locals(): del data_url\n",
    "if 'display_movie' in locals(): del display_movie\n",
    "if 'max_' in locals(): del max_\n",
    "if 'min_' in locals(): del min_\n",
    "if 'mp4' in locals(): del mp4\n",
    "if 'path' in locals():del path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save memory map file for structural channel\n",
    "border_to_0 = 0 if mc.border_nan == 'copy' else mc.border_to_0 \n",
    "fname_new = cm.save_memmap(mc.mmap_file, base_name='memmap_', order='C',\n",
    "                           border_to_0=border_to_0, dview=dview)\n",
    "\n",
    "# apply the shifts to the functional channel and save\n",
    "fname_new2 = mc.apply_shifts_movie([sp[z][f_ch]], save_memmap=True, order = 'C',\n",
    "                                   save_base_name=(Path(sp[z][f_ch]).resolve().parent/'memmap_').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc650188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie of just corrected functional channel\n",
    "\n",
    "anchor = ( , ) # where to anchor the stim bar\n",
    "wd, ht = , # width and height of the stim bar\n",
    "t_stim_st =  # time in seconds of the start of the stim\n",
    "t_stim_end =  # time in seconds of the end of the stim\n",
    "\n",
    "###########################################################\n",
    "fr_stim_st = int(ds_ratio * t_stim_st)\n",
    "fr_stim_end = int(ds_ratio * t_stim_end)\n",
    "\n",
    "mv = cm.load(fname_new2).resize(1, 1, ds_ratio)\n",
    "_min, _max = np.percentile(mv, 0.01), np.percentile(mv, 99.99)\n",
    "mv = np.array(255 * (mv - _min)/(1.1 * (_max - _min)), dtype = 'uint8')\n",
    "mv = mv[:,:,:,None] * np.ones(mv.shape + (3,))\n",
    "mv[fr_stim_st:fr_stim_end, anchor[0]:anchor[0] + ht, anchor[1]:anchor[1] + wd, 0] = 255\n",
    "imageio.mimwrite((Path(sp[z][mc_ch]).parent/f'pre_processed_{speed_up}x_speed.mp4').as_posix(),\n",
    "                 mv.astype('uint8'), fps = fps,  quality=7)\n",
    "del mv\n",
    "\n",
    "mp4 = open((Path(sp[z][mc_ch]).parent/f'pre_processed_{speed_up}x_speed.mp4').as_posix(),'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fname_new)\n",
    "print(fname_new2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c03789",
   "metadata": {},
   "source": [
    "# Source Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e01e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data\n",
    "\n",
    "# fname_new = '/Users/nathanielnyema/Downloads/tmp_ap_2pdata_storage/AP -2-070/ch1_z_1_movie/memmap__d1_512_d2_512_d3_1_order_C_frames_300_.mmap'\n",
    "# fname_new2 = '/Users/nathanielnyema/Downloads/tmp_ap_2pdata_storage/AP -2-070/ch2_z_1_movie/memmap__d1_512_d2_512_d3_1_order_C_frames_300_.mmap'\n",
    "\n",
    "Yr, dims, T = cm.load_memmap(fname_new)\n",
    "images = np.reshape(Yr.T, [T] + list(dims), order='F')\n",
    "\n",
    "Yr, dims, T = cm.load_memmap(fname_new2)\n",
    "images2 = np.reshape(Yr.T, [T] + list(dims), order='F')\n",
    "\n",
    "del Yr\n",
    "del dims\n",
    "del T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde27668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop and restart clusters to clear up memory\n",
    "if 'dview' in locals():\n",
    "    cm.stop_server(dview=dview)\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='local', n_processes=None, single_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01184395",
   "metadata": {},
   "source": [
    "### Use Cellpose to find ROIs to seed the CNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first compute the local correlation map\n",
    "\n",
    "method = 'max'\n",
    "filt = True # whether or not to spatially filter the resulting image\n",
    "kern = 1 # kernel size of filter\n",
    "\n",
    "#############################\n",
    "func = cm.load(fname_new2)\n",
    "\n",
    "if method == 'max':\n",
    "    f_lc = np.percentile(func, 99, axis=0)\n",
    "elif method == 'local_correlation':\n",
    "    f_lc = func.local_correlations()\n",
    "elif method == 'mean':\n",
    "    f_lc = np.mean(func, axis=0)\n",
    "else:\n",
    "    print('unrecognized method')\n",
    "    f_lc = np.zeros(func.shape[1:])\n",
    "    \n",
    "if filt:\n",
    "    f_lc = median_filter(f_lc, kern)\n",
    "    \n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(f_lc)\n",
    "\n",
    "del func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [[0,0]]\n",
    "flow_threshold = 2\n",
    "cellprob_threshold = -1\n",
    "diameter = 15\n",
    "model_type = 'cyto'\n",
    "\n",
    "model = models.Cellpose(model_type='cyto', gpu=False)\n",
    "masks, _, _, _ = model.eval(f_lc,  diameter= diameter, channels=channels, \n",
    "                            flow_threshold = flow_threshold,\n",
    "                            cellprob_threshold = cellprob_threshold)\n",
    "Ain = np.array([(masks==i).flatten('F') for i in np.sort(np.unique(masks))[1:]]).T\n",
    "\n",
    "#create image with masks for GUI purposes\n",
    "im = (f_lc*190//f_lc.max()).astype(np.uint8)\n",
    "im = cv.cvtColor(im,cv.COLOR_GRAY2RGB)\n",
    "ms = np.zeros([*f_lc.shape,3],dtype=np.uint8)\n",
    "for i in Ain.T:\n",
    "    ms[i.reshape(f_lc.shape,order='F')] = np.tile(65*np.random.rand(1,3),(i.sum(),1)).astype(int)\n",
    "\n",
    "#show the initial estimates from CellPose\n",
    "fig, ax=plt.subplots(1,2, figsize = (10,5))\n",
    "ax[0].imshow(im + ms);\n",
    "ax[1].imshow(im);\n",
    "fig.tight_layout(pad = 1.)\n",
    "\n",
    "np.save(Path(sp[z][f_ch]).parent/'Ain.npy', Ain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5a0dd",
   "metadata": {},
   "source": [
    "### A GUI to add and remove ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e195fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we allow the user to decide if they'd like to make edits to the results from cellpose (ie remove or add neurons)\n",
    "#NOTE: press the esc key to exit the GUI!\n",
    "while input('Would you like to remove any neurons (y/n)?  ').lower() in ['y','ye','yes']:\n",
    "    print('Right click on neurons in the viewer that ')\n",
    "    neurons,_ms=remove_neurons(Ain,im,ms.copy())\n",
    "    if input(\"Are you sure you'd like to remove these neurons (y/n)?  \").lower() in ['y','ye','yes']:\n",
    "        print('deleting...')\n",
    "        Ain=np.delete(Ain,neurons,1)\n",
    "        print(neurons)\n",
    "        ms=_ms\n",
    "\n",
    "\n",
    "while input('Would you like to add more neurons (y/n)?  ').lower() in ['y','ye','yes']:\n",
    "    _ms,mask=draw_masks(im,ms.copy(),np.zeros(images.shape[-2:],np.uint8),show_plot=False)\n",
    "    if input(\"Are you sure you'd like to add this neuron (y/n)?  \").lower() in ['y','ye','yes']:\n",
    "        print('adding...')\n",
    "        Ain=np.concatenate([Ain,mask.reshape((-1,1),order='F')],axis=1)\n",
    "        ms=_ms\n",
    "\n",
    "#remove empty masks that were kept\n",
    "Ain=Ain[:,~(Ain.sum(axis=0)==0)]\n",
    "\n",
    "np.save(Path(sp[z][f_ch]).parent/'Ain.npy',Ain)\n",
    "plt.imshow(im+ms);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef05cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ain_file = list(filter( lambda f: 'Ain' in f.name, Path(sp[z][f_ch]).parent.iterdir() ))\n",
    "if len(ain_file)>0:\n",
    "    Ain = np.load(ain_file[0])\n",
    "else:\n",
    "    print('no masks found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87bfe06",
   "metadata": {},
   "source": [
    "### Run CNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109efbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Ain = np.load('/Users/kphuang/Documents/two-photon-analysis/062923 ensure_cinacalcet_LiCl/ensure_cina/ch2_z_2_movie_concat/Ain.npy')\n",
    "np.shape(Ain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters for source extraction\n",
    "opts_dict = {\n",
    "    'fr': fr,\n",
    "    'decay_time': 1.8, # this should be set to 1.8 for GCamp6s, but can be changed to 0.4 for faster indicators\n",
    "    'p': 2, # this must be 2 for GCamp6s, it can be set to 1 for faster indicators\n",
    "    'nb': 2, # the number of background components, \n",
    "    'rf': None, #must be None for seeded mode\n",
    "    'only_init':False, #must be false for seeded mode\n",
    "    'min_SNR': 2.0,\n",
    "    'use_cnn': False, # whether or not to use a convolutional neural network to classify rois as good or bad\n",
    "    'use_cuda':False, # whether or not to use GPUs\n",
    "    ## the rest of these are necessary parameters to set for that get ignored\n",
    "    ## since we are seeding the algorithm for initialization\n",
    "    'K': 300,  \n",
    "    'gSig':[10,10],\n",
    "}\n",
    "\n",
    "opts = params.CNMFParams(params_dict=opts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop and restart clusters to clear up memory\n",
    "if 'dview' in locals():\n",
    "    cm.stop_server(dview=dview)\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='local', n_processes=None, single_thread=False)\n",
    "\n",
    "# #do an initial fit of the cnmf\n",
    "cnm = cnmf.CNMF(n_processes, params=opts, dview=dview, Ain=Ain)\n",
    "cnm = cnm.fit(images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80cd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the initial contours to see if we need to change any parameters\n",
    "Cn = f_lc\n",
    "cnm.estimates.plot_contours(img=Cn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw traces\n",
    "cnm.estimates.dims=cnm.dims\n",
    "cnm.estimates.view_components(img=Cn,idx=[41]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c0538",
   "metadata": {},
   "source": [
    "### Save the cnmf object for later analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebc1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm.save((Path(sp[z][f_ch]).parent/'cnmf.hdf5').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2167896",
   "metadata": {},
   "source": [
    "### Potional step to compute  Δ𝐹/𝐹 here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_df_f(c, baseline, quantileMin = 50, use_residuals = False):\n",
    "    \"\"\"\n",
    "    A custom version of the detrend_df_f function in the original CaImAn code.\n",
    "    In this version, we compute the baseline fluorescence using only a specified number\n",
    "    of frames at the beginning of the recording. By setting quantileMin to 50 we effectively\n",
    "    normalize to the median of the baseline period.\n",
    "    \"\"\"\n",
    "    A, C, b, f, YrA = c.estimates.A, c.estimates.C, c.estimates.b, c.estimates.f, c.estimates.YrA\n",
    "    F = C + YrA if use_residuals else C\n",
    "    F = F * np.sqrt((A.T @ A).diagonal()[:,None])\n",
    "    B = b @ f\n",
    "    f0 =  F + (A.T @ B)\n",
    "    f0 = np.percentile(f0[:,:baseline], quantileMin, axis=1)\n",
    "    fb = np.percentile(F[:,:baseline], quantileMin, axis=1)\n",
    "    df_f = (F - fb[:,None])/f0[:,None]\n",
    "    \n",
    "    return df_f\n",
    "\n",
    "\n",
    "bline = 60\n",
    "fl_acc = custom_df_f(cnm, bline, use_residuals = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955ea78",
   "metadata": {},
   "source": [
    "### Optional step to create a denoised movie of the denoised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a denoised movie of the denoised \n",
    "mv = np.rollaxis((cnm.estimates.A @ cnm.estimates.C).reshape((*cnm.dims,-1), order = 'F'), -1)\n",
    "_min, _max = mv.min(), mv.max()\n",
    "mv = np.array(255* (mv-_min)/(_max-_min), dtype = 'uint8')\n",
    "imageio.mimwrite((Path(sp[z][f_ch]).parent/'denoised.mp4').as_posix(), mv, fps = 30,  quality=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca295b2",
   "metadata": {},
   "source": [
    "# Manually separate subregions and calculate Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ef370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bpl\n",
    "from bokeh.layouts import row\n",
    "from bokeh.plotting import show\n",
    "from bokeh.models import LassoSelectTool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        # this is used for debugging puqrposes only. allows to reload classes\n",
    "        # when changed\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_pick_dots, nb_show_work, get_contours\n",
    "bpl.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60452aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading hdf5 files and stacked images\n",
    "cnm2 = cnmf.load_CNMF('Path')       # load hdf5 file\n",
    "img = plt.imread('Path')            # load stacked image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a6f8c",
   "metadata": {},
   "source": [
    "### Use lasso on left plot to select analysis region 1 (Area Postrema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_indices = [None, None]\n",
    "p1 = nb_pick_dots(img, cnm2.estimates.A, cnm2.estimates.dims[0], cnm2.estimates.dims[1], bg_brightness=0.8, line_width=1, show=True, dot_color='dodgerblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_indices[0] = selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d3676",
   "metadata": {},
   "source": [
    "### Use lasso on left plot to select analysis region 2 (Nucleus of Solitary Tract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fefc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = nb_pick_dots(img, cnm2.estimates.A, cnm2.estimates.dims[0], cnm2.estimates.dims[1], bg_brightness=0.4, line_width=1, show=True, dot_color='dodgerblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_indices[1] = selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d254eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "# All contours in white\n",
    "pcheck = nb_show_work(img, cnm2.estimates.A, cnm2.estimates.dims[0], cnm2.estimates.dims[1], bg_brightness=0.6, line_color='white', show=False)\n",
    "\n",
    "# Area 1 plotted in red\n",
    "coors = get_contours(cnm2.estimates.A[:,area_indices[0]], np.shape(img), thr=None, thr_method='max')\n",
    "cc1 = [np.clip(cor['coordinates'][1:-1, 0], 0, cnm2.estimates.dims[1]) for cor in coors]\n",
    "cc2 = [np.clip(cor['coordinates'][1:-1, 1], 0, cnm2.estimates.dims[0]) for cor in coors]\n",
    "pcheck.patches(cc1, cc2, alpha=1, color=None, line_color='red', line_width=1.5)\n",
    "\n",
    "# Area 2 plotted in yellow\n",
    "coors = get_contours(cnm2.estimates.A[:,area_indices[1]], np.shape(img), thr=None, thr_method='max')\n",
    "cc1 = [np.clip(cor['coordinates'][1:-1, 0], 0, cnm2.estimates.dims[1]) for cor in coors]\n",
    "cc2 = [np.clip(cor['coordinates'][1:-1, 1], 0, cnm2.estimates.dims[0]) for cor in coors]\n",
    "pcheck.patches(cc1, cc2, alpha=1, color=None, line_color='yellow', line_width=1.5)\n",
    "bpl.show(pcheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569cc531",
   "metadata": {},
   "source": [
    "### Calculate dF or Zscore based on specific range for STIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20303a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_df_f_startend(c, baseline_start, baseline_end, method = 'zscore', use_residuals = False):\n",
    "    \"\"\"\n",
    "    A custom version of the detrend_df_f function in the original CaImAn code.\n",
    "    In this version, we compute the baseline fluorescence using only a specified number\n",
    "    of frames at the beginning of the recording. By setting quantileMin to 50 we effectively\n",
    "    normalize to the median of the baseline period.\n",
    "    \"\"\"\n",
    "    A, C, b, f, YrA = c.estimates.A, c.estimates.C, c.estimates.b, c.estimates.f, c.estimates.YrA\n",
    "    F = C + YrA if use_residuals else C\n",
    "    F = F * np.sqrt((A.T @ A).diagonal()[:,None])\n",
    "    B = b @ f\n",
    "    f0 =  F + (A.T @ B)\n",
    "    if method == 'norm_to_median':\n",
    "        f0 = np.percentile(f0[:,baseline_start:baseline_end], 50, axis=1)\n",
    "        fb = np.percentile(F[:,baseline_start:baseline_end], 50, axis=1)\n",
    "    elif method == 'zscore':\n",
    "        fb = np.mean(F[:,baseline_start:baseline_end], axis=1)\n",
    "        f0 = np.std(f0[:,baseline_start:baseline_end], axis=1)\n",
    "    df_f = (F - fb[:,None])/f0[:,None]\n",
    "    \n",
    "    return df_f\n",
    "\n",
    "method = 'zscore'\n",
    "use_residuals = True\n",
    "bline_start =     # Enter the baseline_start for this stimulation\n",
    "bline_end =       # Enter the baseline_end for this stimulation\n",
    "fl_acc = custom_df_f_startend(cnm2, bline_start, bline_end, method = method, \n",
    "                     use_residuals = use_residuals )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim1_start =     # Enter the stim_start for this stimulation\n",
    "stim1_end =       # Enter the stim_end for this stimulation\n",
    "\n",
    "stim1_area1_data = fl_acc[area_indices[0],stim1_start:stim1_end]\n",
    "stim1_area2_data = fl_acc[area_indices[1],stim1_start:stim1_end]\n",
    "\n",
    "np.savetxt('stim1_AP.csv', stim1_area1_data, delimiter=',')\n",
    "np.savetxt('stim1_NTS.csv', stim1_area2_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c244efcb",
   "metadata": {},
   "source": [
    "# Optional Step to calculate difference betweem stimuli and create colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
    "from caiman.base.rois import com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_df_f_startend(c, baseline_start, baseline_end, method = 'zscore', use_residuals = False):\n",
    "    \"\"\"\n",
    "    A custom version of the detrend_df_f function in the original CaImAn code.\n",
    "    In this version, we compute the baseline fluorescence using only a specified number\n",
    "    of frames at the beginning of the recording. By setting quantileMin to 50 we effectively\n",
    "    normalize to the median of the baseline period.\n",
    "    \"\"\"\n",
    "    A, C, b, f, YrA = c.estimates.A, c.estimates.C, c.estimates.b, c.estimates.f, c.estimates.YrA\n",
    "    F = C + YrA if use_residuals else C\n",
    "    F = F * np.sqrt((A.T @ A).diagonal()[:,None])\n",
    "    B = b @ f\n",
    "    f0 =  F + (A.T @ B)\n",
    "    if method == 'norm_to_median':\n",
    "        f0 = np.percentile(f0[:,baseline_start:baseline_end], 50, axis=1)\n",
    "        fb = np.percentile(F[:,baseline_start:baseline_end], 50, axis=1)\n",
    "    elif method == 'zscore':\n",
    "        fb = np.mean(F[:,baseline_start:baseline_end], axis=1)\n",
    "        f0 = np.std(f0[:,baseline_start:baseline_end], axis=1)\n",
    "    df_f = (F - fb[:,None])/f0[:,None]\n",
    "    \n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbd96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm2 = cnmf.load_CNMF('Path')         # Load hdf5 file\n",
    "img = plt.imread('Path')              # Load stacked image\n",
    "center = com(cnm2.estimates.A, cnm2.estimates.dims[0], cnm2.estimates.dims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39762a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the baseline and stimulation time for each stimulation\n",
    "# STIM 1:\n",
    "baseline_1 = [ , ]\n",
    "stim_1 = [ , ]\n",
    "\n",
    "# STIM 2:\n",
    "baseline_2 = [ , ]\n",
    "stim_2 = [ , ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a208ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'zscore'\n",
    "use_residuals = True\n",
    "\n",
    "fl_acc_1 = custom_df_f_startend(cnm2, baseline_1[0], baseline_1[1], method = method, \n",
    "                     use_residuals = use_residuals )\n",
    "\n",
    "stim_mdn_1 = np.median(fl_acc_1[:,stim_1[0]:stim_1[1]], axis=1)\n",
    "\n",
    "fl_acc_2 = custom_df_f_startend(cnm2, baseline_2[0], baseline_2[1], method = method, \n",
    "                     use_residuals = use_residuals )\n",
    "\n",
    "stim_mdn_2 = np.median(fl_acc_2[:,stim_2[0]:stim_2[1]], axis=1)\n",
    "\n",
    "# Index.cinacalcet is [Stim 2 - Stim 1]\n",
    "stim_idx = stim_mdn_2 - stim_mdn_1\n",
    "stim_idx[np.maximum(stim_mdn_1, stim_mdn_2)<1.64] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap='gray',alpha=1)\n",
    "# you could change edgecolor='none' to edgecolor='black' if you want to try lines around the circles\n",
    "plt.scatter(center[:,1],center[:,0],c=stim_idx, cmap='bwr', vmin=-10, vmax=10, alpha=1, s=50, edgecolor='none',linewidth=0.75)\n",
    "plt.colorbar()\n",
    "fig = plt.gcf()\n",
    "fig.savefig('cinacalcet.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caiman",
   "language": "python",
   "name": "caiman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
